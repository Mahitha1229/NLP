{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9232ab6f",
   "metadata": {},
   "source": [
    "## Telugu Bigram Language Modeling with Smoothing Techniques\n",
    "A Modular Implementation Bigram model using tokenized_telugu.txt\n",
    "\n",
    "# This title reflects:\n",
    "- The core technique (Add-One smoothing)\n",
    "- The language and dataset (Telugu corpus)\n",
    "- The scope (-bigram model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1ed7d",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Begin by importing the collections module, which provides efficient data structures for counting n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fcea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099648b0",
   "metadata": {},
   "source": [
    "## Load the Tokenized Corpus\n",
    "This function reads the tokenized Telugu corpus line by line, splits each sentence into tokens, and returns a list of token lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9469a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized corpus\n",
    "def load_corpus(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    corpus = [line.strip().split() for line in lines if line.strip()]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692a70a",
   "metadata": {},
   "source": [
    "## Build Bigram and Unigram Counts\n",
    "We prepend `<s>` and append `</s>` to each sentence to capture sentence boundaries. Then we count bigrams and unigrams using collections.Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb51013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram and unigram counts\n",
    "def build_ngram_counts(corpus):\n",
    "    bigram_counts = collections.Counter()\n",
    "    unigram_counts = collections.Counter()\n",
    "    for sentence in corpus:\n",
    "        tokens = ['<s>'] + sentence + ['</s>']\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "            unigram_counts[tokens[i]] += 1\n",
    "    return bigram_counts, unigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35df247",
   "metadata": {},
   "source": [
    "## Define Smoothing Functions\n",
    "Three techniques are implemented:\n",
    "- Add-One: Laplace smoothing\n",
    "- Add-K: Generalized additive smoothing\n",
    "- Token-Type: Heuristic weight-based smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing functions\n",
    "def add_one_smoothing(count, prefix_count, vocab_size):\n",
    "    return (count + 1) / (prefix_count + vocab_size)\n",
    "\n",
    "def add_k_smoothing(count, prefix_count, vocab_size, k):\n",
    "    return (count + k) / (prefix_count + k * vocab_size)\n",
    "\n",
    "def token_type_smoothing(count, token, bigram_counts, token_type_weights):\n",
    "    weight = token_type_weights.get(token, 1)\n",
    "    total = sum(bigram_counts.values()) + sum(token_type_weights.values())\n",
    "    return (count + weight) / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4d156",
   "metadata": {},
   "source": [
    "## Main Workflow\n",
    "Loads corpus, builds counts, computes vocabulary size, applies smoothing, and writes results to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf46e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main workflow\n",
    "def main():\n",
    "    filepath = 'tokenized_telugu.txt'\n",
    "    corpus = load_corpus(filepath)\n",
    "    bigram_counts, unigram_counts = build_ngram_counts(corpus)\n",
    "    vocab = set(token for sentence in corpus for token in sentence)\n",
    "    vocab_size = len(vocab)\n",
    "    k = 0.3\n",
    "\n",
    "    # Heuristic token type weights (can be customized)\n",
    "    token_type_weights = {token: 1.5 for token in vocab}\n",
    "\n",
    "    # Write output with formulas and results\n",
    "    with open('smoothing_output.txt', 'w', encoding='utf-8') as out:\n",
    "        out.write(\"### Smoothing Techniques Applied to Telugu Bigram Corpus\\n\")\n",
    "        out.write(\"This file contains smoothed probabilities for each bigram using three techniques:\\n\\n\")\n",
    "        out.write(\"1. Add-One Smoothing:\\n\")\n",
    "        out.write(\"   P(w₂ | w₁) = (count(w₁, w₂) + 1) / (count(w₁) + V)\\n\")\n",
    "        out.write(\"2. Add-K Smoothing (k = 0.3):\\n\")\n",
    "        out.write(\"   P(w₂ | w₁) = (count(w₁, w₂) + k) / (count(w₁) + k × V)\\n\")\n",
    "        out.write(\"3. Token Type Smoothing:\\n\")\n",
    "        out.write(\"   P(w₂ | w₁) = (count(w₁, w₂) + weight(w₂)) / (∑ bigram_counts + ∑ token_type_weights)\\n\\n\")\n",
    "        out.write(\"Bigram\\tCount\\tPrefixCount\\tAddOne\\tAddK\\tTokenType\\n\")\n",
    "\n",
    "        for bigram in bigram_counts:\n",
    "            count = bigram_counts[bigram]\n",
    "            prefix = bigram[0]\n",
    "            token = bigram[1]\n",
    "            prefix_count = unigram_counts.get(prefix, 0)\n",
    "\n",
    "            p_add_one = add_one_smoothing(count, prefix_count, vocab_size)\n",
    "            p_add_k = add_k_smoothing(count, prefix_count, vocab_size, k)\n",
    "            p_token_type = token_type_smoothing(count, token, bigram_counts, token_type_weights)\n",
    "\n",
    "            out.write(f\"{bigram}\\t{count}\\t{prefix_count}\\t{p_add_one:.6f}\\t{p_add_k:.6f}\\t{p_token_type:.6f}\\n\")\n",
    "\n",
    "    print(\"✅ Smoothing results saved to 'smoothing_output.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f2c85",
   "metadata": {},
   "source": [
    "## Execute the Main Function\n",
    "Runs the full pipeline and saves the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
